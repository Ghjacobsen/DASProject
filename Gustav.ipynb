{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de409e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta,timezone\n",
    "import glob\n",
    "import os\n",
    "from scipy.fft import fft2, ifft2, fftshift, ifftshift\n",
    "from scipy.signal import butter, filtfilt, firwin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal.windows import tukey\n",
    "from scipy.interpolate import interp1d\n",
    "# from matplotlib.colors import LinearSegmentedColomap\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from Helpers import *\n",
    "\n",
    "# Set up matplotlib for inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "323fbd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "Metadata from first file:\n",
      "Start time (Unix timestamp): 1751155206.152\n",
      "Start time (UTC): 2025-06-29 00:00:06.152000+00:00\n",
      "Sampling interval (dt): 0.0016 seconds\n",
      "Spatial sampling (dx): 1.0213001907746815 meters\n",
      "Number of channels: 13752\n",
      "Samples per file: 6250\n",
      "Sampling frequency: 625.0 Hz\n",
      "File duration: 10.0 seconds\n",
      "Total files available: 115\n",
      "Maximum duration available: 1150.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing your HDF5 files\n",
    "directory= \"data/GC_data\"  # Change this to your directory path\n",
    "\n",
    "\n",
    "# Get all HDF5 files in the directory\n",
    "file_paths = sorted(glob.glob(os.path.join(directory, '*.hdf5')))\n",
    "print(len(file_paths))\n",
    "if not file_paths:\n",
    "    print(f\"No HDF5 files found in {directory}\")\n",
    "else:\n",
    "    # Extract metadata from the first file\n",
    "    start_time, dt, dx, channels, num_samples = extract_metadata(file_paths[0])\n",
    "    fs = 1/dt\n",
    "\n",
    "    print(\"Metadata from first file:\")\n",
    "    print(f\"Start time (Unix timestamp): {start_time}\")\n",
    "    print(f\"Start time (UTC): {datetime.fromtimestamp(start_time, tz=timezone.utc)}\")\n",
    "    print(f\"Sampling interval (dt): {dt} seconds\")\n",
    "    print(f\"Spatial sampling (dx): {dx} meters\")\n",
    "    print(f\"Number of channels: {len(channels)}\")\n",
    "    print(f\"Samples per file: {num_samples}\")\n",
    "    print(f\"Sampling frequency: {fs} Hz\")\n",
    "    print(f\"File duration: {num_samples * dt} seconds\")\n",
    "    print(f\"Total files available: {len(file_paths)}\")\n",
    "    print(f\"Maximum duration available: {len(file_paths) * num_samples * dt} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ed01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50 files (500 seconds)\n",
      "Raw data shape: (312500, 13752)\n",
      "Time points: 312500\n",
      "Distance points: 13752\n",
      "Total duration: 500.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Parameters for data processing\n",
    "dmin_km = 0      # Minimum distance in km\n",
    "dmax_km = 30   # Maximum distance in km\n",
    "chunk_size = 10   # CONTROL THIS: Number of files to process (each file = 10 seconds)\n",
    "sample_step = 1 # Temporal downsampling factor jo færrere den her værdi desto langsommere, jo højere desto hurtigre og mere urpæcist.\n",
    "channel_step = 10 # Spatial downsampling factor hvor meget detalje der er med, vi beholder flere kanaler langs kabelet. der er måske 30000.\n",
    "\n",
    "# Process first file to get parameters\n",
    "start_time, dt, dx, channels, _ = extract_metadata(file_paths[0])\n",
    "fs = 1/dt\n",
    "# Create distance axis\n",
    "distance_array_m = (channels) * dx\n",
    "distance_array_km = distance_array_m / 1000\n",
    "dist_mask = (distance_array_km >= dmin_km) & (distance_array_km <= dmax_km)\n",
    "distances = distance_array_km[dist_mask][::channel_step]\n",
    "\n",
    "# Initialize data storage\n",
    "all_data = []\n",
    "time_axis = []\n",
    "\n",
    "# Process multiple files based on chunk_size\n",
    "for file_idx in range(min(chunk_size, len(file_paths))):\n",
    "    file_path = file_paths[file_idx]\n",
    "\n",
    "    file_start_time, file_dt, _, _, file_num_samples = extract_metadata(file_path)\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        data = f['data'][::sample_step, dist_mask][:, ::channel_step]\n",
    "\n",
    "        # Create proper time axis for this file\n",
    "        num_samples = data.shape[0]\n",
    "        file_duration = num_samples * file_dt * sample_step\n",
    "\n",
    "        # Calculate absolute times for this file\n",
    "        # file_start_datetime = datetime.utcfromtimestamp(file_start_time)\n",
    "        file_start_datetime = datetime.fromtimestamp(file_start_time, tz=timezone.utc)\n",
    "\n",
    "        time_deltas = np.arange(0, num_samples) * (file_dt * sample_step)\n",
    "        file_times = [file_start_datetime + timedelta(seconds=float(t)) for t in time_deltas]\n",
    "\n",
    "        all_data.append(data)\n",
    "        time_axis.extend(file_times)\n",
    "\n",
    "# Combine data from all files\n",
    "if len(all_data) > 1:\n",
    "    combined_data = np.vstack(all_data)\n",
    "else:\n",
    "    combined_data = all_data[0]\n",
    "\n",
    "print(f\"Processing {chunk_size} files ({chunk_size * 10} seconds)\")\n",
    "print(f\"Raw data shape: {combined_data.shape}\")\n",
    "#print(f\"Filtered data shape: {filtered_data.shape}\")\n",
    "print(f\"Time points: {len(time_axis)}\")\n",
    "print(f\"Distance points: {len(distances)}\")\n",
    "print(f\"Total duration: {(time_axis[-1] - time_axis[0]).total_seconds():.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a580e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot raw data (time-distance plot)\n",
    "plt.figure(figsize=(14, 7))  # Wider figure for longer durations\n",
    "vmax_amp = np.percentile(np.abs(combined_data), 99)\n",
    "\n",
    "\n",
    "# Set extent with distances increasing from bottom to top\n",
    "img = plt.imshow(abs(combined_data),\n",
    "               aspect='auto',\n",
    "               extent=[distances[0], distances[-1],\n",
    "                       mdates.date2num(time_axis[0]),\n",
    "                       mdates.date2num(time_axis[-1])],\n",
    "               cmap='jet',\n",
    "               vmin=-0,\n",
    "               vmax=vmax_amp)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.yaxis_date()\n",
    "ax.yaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.ylabel('Time')\n",
    "plt.xlabel('Distance (km)')\n",
    "duration_seconds = (time_axis[-1] - time_axis[0]).total_seconds()\n",
    "plt.title(f'Raw DAS Data ({dmin_km}-{dmax_km} km)\\n'\n",
    "          f'Duration: {duration_seconds:.1f}s ({chunk_size} files)\\n'\n",
    "          f'Downsampled: time×{sample_step}, space×{channel_step}')\n",
    "plt.colorbar(img, label='rad/(s*m)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
